{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrappinmg Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from selenium) (0.19.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rk1066\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.12.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing the selenium library\n",
    "\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver #importing webdriver from selenium\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1). Write a python program which searches all the product under a particular product from www.amazon.in. \n",
    "### The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search \n",
    "### for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://www.amazon.in/'\n",
    "driver.get(url) # getting the url\n",
    "time.sleep(4)\n",
    "\n",
    "search_bar=driver.find_element_by_id('twotabsearchtextbox') # locating the seacrh bar by id\n",
    "search_bar.clear()\n",
    "search_item=input('Enter your search item ') # creating the variable to take input from the user\n",
    "search_bar.send_keys(search_item)\n",
    "button=driver.find_element_by_xpath(\"//span[@id='nav-search-submit-text']\") # locating the button by x path\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the empty lists\n",
    "brand_name=[]\n",
    "product_name=[]\n",
    "price=[]\n",
    "exchange=[]\n",
    "expe_del=[]\n",
    "availability=[]\n",
    "url=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the product name by xpath\n",
    "p_name=driver.find_elements_by_xpath('//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\"]/a/span')\n",
    "for i in p_name:\n",
    "    if i.text is None :\n",
    "        product_name.append('--')\n",
    "    else:\n",
    "        product_name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the brand name by xpath\n",
    "brand_name=[]\n",
    "b_name=driver.find_elements_by_xpath('//h5[@class=\"s-line-clamp-1\"]/span')\n",
    "for i in b_name:\n",
    "    if i.text is None :\n",
    "        brand_name.append('--')\n",
    "    else:\n",
    "        brand_name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the Price by xpath\n",
    "price=[]\n",
    "p_name=driver.find_elements_by_xpath('//a[@class=\"a-size-base a-link-normal s-link-style a-text-normal\"]/span')\n",
    "for i in p_name:\n",
    "    if i.text is None :\n",
    "        price.append('--')\n",
    "    else:\n",
    "        price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the return/exchange by xpath\n",
    "availability=[]\n",
    "a_name=driver.find_elements_by_xpath('//span[@class=\"a-size-base a-color-price\"]')\n",
    "for i in a_name:\n",
    "    if i.text is None :\n",
    "        availability.append('--')\n",
    "    else:\n",
    "        availability.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the product URL by xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=[]\n",
    "url_name=driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-no-outline\"]')\n",
    "for i in url_name:\n",
    "    if i.text is None :\n",
    "        url.append('--')\n",
    "    else:\n",
    "        url.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrappin the expiry details by xpath\n",
    "expe_del=[]\n",
    "exp_name=driver.find_elements_by_xpath('//div[@class=\"a-row s-align-children-center\"]/span[2]')\n",
    "for i in exp_name:\n",
    "    if i.text is None :\n",
    "        expe_del.append('--')\n",
    "    else:\n",
    "        expe_del.append(i.get_attribute('aria-label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expe_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrappin the return/exchange by xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapping the next page button\n",
    "nxt_button=driver.find_elements_by_xpath('//span[@class=\"s-pagination-strip\"]/a')\n",
    "try:\n",
    "    driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "except:\n",
    "    driver.get(nxt_button[0].get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amazon product details\n",
    "\n",
    "#creating empty list\n",
    "brand_name=[]\n",
    "product_name=[]\n",
    "price=[]\n",
    "exchange=[]\n",
    "expe_del=[]\n",
    "availability=[]\n",
    "url=[]\n",
    "\n",
    "start =0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    b_name=driver.find_elements_by_xpath('//h5[@class=\"s-line-clamp-1\"]/span')\n",
    "    for i in b_name:\n",
    "        if i.text is None :\n",
    "            brand_name.append('--')\n",
    "        else:\n",
    "            brand_name.append(i.text)\n",
    "    p_name=driver.find_elements_by_xpath('//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\"]/a/span')\n",
    "    for i in p_name:\n",
    "        if i.text is None :\n",
    "            product_name.append('--')\n",
    "        else:\n",
    "            product_name.append(i.text)\n",
    "    p_name=driver.find_elements_by_xpath('//a[@class=\"a-size-base a-link-normal s-link-style a-text-normal\"]/span')\n",
    "    for i in p_name:\n",
    "        if i.text is None :\n",
    "            price.append('--')\n",
    "        else:\n",
    "            price.append(i.text)\n",
    "    \n",
    "    exp_name=driver.find_elements_by_xpath('//div[@class=\"a-row s-align-children-center\"]/span[2]')\n",
    "    for i in exp_name:\n",
    "        if i.text is None :\n",
    "            expe_del.append('--')\n",
    "        else:\n",
    "            expe_del.append(i.get_attribute('aria-label'))\n",
    "    \n",
    "    a_name=driver.find_elements_by_xpath('//span[@class=\"a-size-base a-color-price\"]')\n",
    "    for i in a_name:\n",
    "        if i.text is None :\n",
    "            availability.append('--')\n",
    "        else:\n",
    "            availability.append(i.text)\n",
    "    url_name=driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-no-outline\"]')\n",
    "    for i in url_name:\n",
    "        if i.text is None :\n",
    "            url.append('--')\n",
    "        else:\n",
    "            url.append(i.get_attribute('href'))\n",
    "    nxt_button=driver.find_elements_by_xpath('//span[@class=\"s-pagination-strip\"]/a')\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe\n",
    "\n",
    "amazon_details =pd.DataFrame({\"Brand\":brand_name[:2],\"Product\":product_name[:2],\"Price\":price[:2],\"Expected delivery\":expe_del[:2],\"Availability\":availability[:2],\"URL\":url[:2]})\n",
    "amazon_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3) Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "# images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the webdriver\n",
    "\n",
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "\n",
    "driver =webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the webpage\n",
    "driver.get('https://images.google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the search button\n",
    "search_bar=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')#finding the xpath\n",
    "search_item=input('Enter your search item ')\n",
    "search_bar.send_keys(search_item)\n",
    "search_btn=driver.find_element_by_xpath(\"//*[@id='sbtc']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=[]\n",
    "images=driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "for i in images:\n",
    "    if i.get_attribute('src') and 'http' in i.get_attribute('src'):\n",
    "        img.append(i.get_attribute('src'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the webdriver\n",
    "\n",
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)\n",
    "# accessing the webpage\n",
    "driver.get('https://images.google.com')\n",
    "# Accessing the search button\n",
    "search_bar=driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')#finding the xpath\n",
    "#serach_bar.clear()\n",
    "search_item=input('Enter your search item ')\n",
    "search_bar.send_keys(search_item)\n",
    "search_btn=driver.find_element_by_xpath(\"//*[@id='sbtc']/button\")\n",
    "search_btn.click()\n",
    "time.sleep(4)\n",
    "#scrapping the image urls by x path\n",
    "images=driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "# creating the empty list\n",
    "img=[]\n",
    "for i in images:\n",
    "    if i.get_attribute('src') and 'http' in i.get_attribute('src'):\n",
    "        img.append(i.get_attribute('src'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4) Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) onwww.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the webdriver\n",
    "\n",
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)\n",
    "# accessing the webpage\n",
    "driver.get('http://www.flipkart.com/')\n",
    "# Accessing the search button\n",
    "search_bar=driver.find_element_by_xpath('//div[@class=\"col-12-12 _2oO9oE\"]/div/input')#finding the xpath\n",
    "#serach_bar.clear()\n",
    "search_item=input('Enter your search item ')\n",
    "search_bar.send_keys(search_item)\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the image urls by x path\n",
    "brand_name=[]\n",
    "b_name=driver.find_elements_by_xpath('//div[@class=\"_4rR01T\"]')\n",
    "for i in b_name:\n",
    "    brand_name.append(i.text.split()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_name=[]\n",
    "s_name=driver.find_elements_by_xpath('//div[@class=\"_4rR01T\"]')\n",
    "for i in s_name:\n",
    "    smart_name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_name"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_urls = []\n",
    "urls = driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "for url in urls:\n",
    "    flip_urls.append(url.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flip_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_dict = {}\n",
    "flip_dict[\"Brand\"] = []\n",
    "flip_dict[\"Smartphone\"] = []\n",
    "flip_dict[\"Colour\"] = []\n",
    "flip_dict[\"RAM\"] = []\n",
    "flip_dict[\"Storage(ROM)\"] = []\n",
    "flip_dict[\"Primary Camera\"] = []\n",
    "flip_dict[\"Secondary Camera\"] = []\n",
    "flip_dict[\"Display Size\"] = []\n",
    "flip_dict[\"Display Resolution\"] = []\n",
    "flip_dict[\"Processor\"] = []\n",
    "flip_dict[\"Processor Cores\"] = []\n",
    "flip_dict[\"Battery Capacity\"] = []\n",
    "flip_dict[\"Battery Type\"] = []\n",
    "flip_dict[\"Price\"] = []\n",
    "flip_dict[\"URL\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in flip_urls:\n",
    "    driver.get(url)    # Saving url                                                     \n",
    "    print(\"Scraping URL = \", url)\n",
    "    flip_dict['URL'].append(url)                                                          # Loading the webpage by url\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        read_more = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')     # Button for expanding the specs\n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception Occured. Moving to next page\")\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')      # Extracting Brand from xpath\n",
    "        flip_dict[\"Brand\"].append(brand.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Brand'].append('-')\n",
    "        \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')      # Extracting Brand from xpath\n",
    "        flip_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Price'].append('-')\n",
    "        \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')      # Extracting Brand from xpath\n",
    "        flip_dict['Smartphone'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Smartphone'].append('-')\n",
    "    \n",
    "    try:\n",
    "        color = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      # Extracting Name from xpath\n",
    "        flip_dict['Colour'].append(color.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Colour'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_size = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[1]/td[2]/ul/li')  # Extracting Ratings from xpath\n",
    "        flip_dict['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Size'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_res = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[2]/td[2]/ul/li')     # Extracting no. of Ratings from xpath\n",
    "        flip_dict['Display Resolution'].append(disp_res.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Resolution'].append('-')\n",
    "    \n",
    "    try:\n",
    "        pro_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "        if pro_chk.text != \"Processor Type\" : raise NoSuchElementException\n",
    "        processor = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')   # Extracting Price from xpath\n",
    "        flip_dict['Processor'].append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor'].append('-')\n",
    "    \n",
    "    try:                                                                                     # Extracting Return/Exchange policy from xpath\n",
    "        core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[1]')\n",
    "        if core_chk.text != \"Processor Core\" :\n",
    "            core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "            if core_chk.text != \"Processor Core\" : \n",
    "                raise NoSuchElementException\n",
    "            else :\n",
    "                cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        else :\n",
    "            cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[2]/ul/li')\n",
    "        flip_dict['Processor Cores'].append(cores.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor Cores'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li')         # Extracting Expected Delivery from xpath\n",
    "        flip_dict['Storage(ROM)'].append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Storage(ROM)'].append('-')\n",
    "    \n",
    "    try:\n",
    "        ram = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['RAM'].append(ram.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['RAM'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Other Details from xpath\n",
    "        pri_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        flip_dict['Primary Camera'].append(pri_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Primary Camera'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Other Details from xpath\n",
    "        cam_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[1]')\n",
    "        if cam_chk != \"Secondary Camera\" : \n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[1]').text == \"Secondary Camera\":\n",
    "                sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li')\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[2]/ul/li')\n",
    "        flip_dict['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Secondary Camera'].append('-')\n",
    "        \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[1]')\n",
    "            if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['Battery Capacity'].append(bat_cap.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Capacity'].append('-')\n",
    "    \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[1]')\n",
    "            if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "            bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['Battery Type'].append(bat_typ.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Type'].append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_df = pd.DataFrame.from_dict(flip_dict)\n",
    "flip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5)Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google\n",
    "## maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "# locating search bar\n",
    "search = driver.find_element_by_id(\"searchboxinput\") \n",
    "# clearing search bar\n",
    "search.clear()                                                            \n",
    "time.sleep(2)\n",
    "# entering values in search bar\n",
    "search_item=input(\"enter location\")\n",
    "search.send_keys(search_item) \n",
    "# locating search button\n",
    "button = driver.find_element_by_id(\"searchbox-searchbutton\")\n",
    "# clicking search button\n",
    "button.click()                                                          \n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q) 6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) \n",
    "## from trak.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the webdriver\n",
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)\n",
    "driver.get(\"https://trak.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " fund_button   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " driver.get(fund_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty list and dictionary\n",
    "fund_dict = {}\n",
    "fund_dict['Date'] = []\n",
    "fund_dict['Startup Name'] = []\n",
    "fund_dict['Industry/Vertical'] = []\n",
    "fund_dict['Sub-Vertical'] = []\n",
    "fund_dict['Location'] = []\n",
    "fund_dict['Investor'] = []\n",
    "fund_dict['Investment Type'] = []\n",
    "fund_dict['Amount(in USD)'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(54,57):\n",
    "    driver.find_element_by_xpath('//div[@id=\"tablepress-{}_wrapper\"]/div/label/select/option[1]'.format(i)).click()\n",
    "\n",
    "    # Date\n",
    "    dt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for d in dt:\n",
    "        fund_dict['Date'].append(d.text)\n",
    "\n",
    "    # Startup Name\n",
    "    sn = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for n in sn:\n",
    "        fund_dict['Startup Name'].append(n.text)\n",
    "    \n",
    "    # Industry/Vertical\n",
    "    ind = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind:\n",
    "        fund_dict['Industry/Vertical'].append(n.text)\n",
    "  # Sub-Vertical\n",
    "    sv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for s in sv:\n",
    "        fund_dict['Sub-Vertical'].append(s.text)\n",
    "\n",
    "    # Location\n",
    "    loc = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for l in loc:\n",
    "        fund_dict['Location'].append(l.text)\n",
    "# Investor\n",
    "    inv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for n in inv:\n",
    "        fund_dict['Investor'].append(n.text)\n",
    "    \n",
    "    # Investment Type\n",
    "    invt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for n in invt:\n",
    "        fund_dict['Investment Type'].append(n.text)\n",
    "    \n",
    "    # Amount\n",
    "    amt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for a in amt:\n",
    "        fund_dict['Amount(in USD)'].append(a.text)\n",
    "    \n",
    "fund_df = pd.DataFrame(fund_dict)\n",
    "fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the webdriver\n",
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)\n",
    "driver.get(\"http://www.digit.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_button = driver.find_element_by_xpath('//div[@class=\"menu\"]/ul/li[3]/a').get_attribute('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(laptops_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best gaming laptop link\n",
    "gaming_lap=driver.find_element_by_xpath('//div[@class=\"Listbrand\"]/ul/li[10]/a').get_attribute('href')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaming_lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(gaming_lap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the top 10 gaming laptop details\n",
    "product_name=[] # extracting product name\n",
    "prd_name= driver.find_elements_by_xpath('//div[@class=\"Top10-Seller\"]//table/tbody/tr/td[1]'.format(i))\n",
    "for i in prd_name:\n",
    "    product_name.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_name=[] # extracting seller details\n",
    "sel_name= driver.find_elements_by_xpath('//div[@class=\"Top10-Seller\"]//table/tbody/tr/td[2]'.format(i))\n",
    "for i in sel_name:\n",
    "    seller_name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price=[] # extracting price details\n",
    "price_det= driver.find_elements_by_xpath('//div[@class=\"Top10-Seller\"]//table/tbody/tr/td[3]'.format(i))\n",
    "for i in price_det:\n",
    "    price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaming_laptops=pd.DataFrame({\"Product Name\":product_name,\"Seller Name\":seller_name,\"Price\":price})\n",
    "Gaming_laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be \n",
    "## scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the webdriver\n",
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)\n",
    "driver.get(\"https://www.forbes.com/billionaires/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the details\n",
    "# Rank\n",
    "Rank=[]\n",
    "rank=driver.find_elements_by_xpath('//div[@class=\"rank\"]'.format(i))\n",
    "for i in rank:\n",
    "    Rank.append(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name\n",
    "Name=[]\n",
    "name=driver.find_elements_by_xpath('//div[@class=\"personName\"]'.format(i))\n",
    "for i in name:\n",
    "    Name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Net worth\n",
    "Net_worth=[]\n",
    "net=driver.find_elements_by_xpath('//div[@class=\"netWorth\"]'.format(i))\n",
    "for i in net:\n",
    "    Net_worth.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net_worth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age\n",
    "Age=[]\n",
    "age=driver.find_elements_by_xpath('//div[@class=\"age\"]/div'.format(i))\n",
    "for i in age:\n",
    "    Age.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citizenship\n",
    "Citizenship=[]\n",
    "ctz=driver.find_elements_by_xpath('//div[@class=\"countryOfCitizenship\"]'.format(i))\n",
    "for i in ctz:\n",
    "    Citizenship.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Citizenship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "Source=[]\n",
    "src=driver.find_elements_by_xpath('//div[@class=\"source\"]'.format(i))\n",
    "for i in src:\n",
    "    Source.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Industry\n",
    "Industry=[]\n",
    "inds=driver.find_elements_by_xpath('//div[@class=\"category\"]'.format(i))\n",
    "for i in inds:\n",
    "    Industry.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forbes_details=pd.DataFrame({\"Rank\":Rank,\"Name\":Name,\"Net Worth\":Net_worth,\"Age\":Age,\"Citizenship\":Citizenship,\"Source\":Source,\"Industry\":Industry})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forbes_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the webdriver\n",
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)\n",
    "driver.get(\"https://www.youtube.com/watch?v=UF8uR6Z6KLc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments\n",
    "Comment=[]\n",
    "comment=driver.find_elements_by_xpath('//*[@id=\"content-text\"]')\n",
    "for i in comment:\n",
    "    Comment.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "i=0\n",
    "if i<=10:\n",
    "#comments\n",
    "    Comment=[]\n",
    "    comment=driver.find_elements_by_xpath('//*[@id=\"content-text\"]')\n",
    "    for i in comment:\n",
    "        Comment.append(i.text)\n",
    "\n",
    "# Comment upvote\n",
    "    Upvote=[]\n",
    "    upvote=driver.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')\n",
    "    try:\n",
    "        if i is not None:\n",
    "            for i in upvote:\n",
    "                Upvote.append(i.text)\n",
    "        else:\n",
    "            Upvote.append(\"zero upvotes\")\n",
    "    except NoSuchElementException :\n",
    "        Upvote.append(\"zero upvotes\")\n",
    "#time\n",
    "    Time=[]\n",
    "    time=driver.find_elements_by_xpath('//yt-formatted-string[@class=\"published-time-text above-comment style-scope ytd-comment-renderer\"]/a'.format(i))\n",
    "    for i in time:\n",
    "        Time.append(i.get_attribute('dir'))\n",
    "#scroll down to load comments\n",
    "driver.execute_script('window.scrollTo(0,390);')\n",
    "time.sleep(4)\n",
    "   \n",
    "\n",
    "#YouTube_comments=pd.DataFrame({\"Comments\":Comment[500],\"Upvotes\":Upvote[500],\"Time\":Time[500]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment upvote\n",
    "Upvote=[]\n",
    "upvote=driver.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')\n",
    "for i in upvote:\n",
    "    try:\n",
    "        if i is not None:\n",
    "            Upvote.append(i.text)\n",
    "                \n",
    "        else:\n",
    "            Upvote.append(\"zero upvotes\")\n",
    "    except NoSuchElementException :\n",
    "        Upvote.append(\"zero upvotes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Upvote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time\n",
    "Time=[]\n",
    "time=driver.find_elements_by_xpath('//yt-formatted-string[@class=\"published-time-text above-comment style-scope ytd-comment-renderer\"]/a'.format(i))\n",
    "for i in time:\n",
    "    Time.append(i.get_attribute('dir'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTube_comments=pd.DataFrame({\"Comments\":Comment,\"Upvotes\":Upvote,\"Time\":Time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTube_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the webdriver\n",
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)\n",
    "driver.get(\"https://www.hostelworld.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(4)\n",
    "\n",
    "search_bar=driver.find_element_by_xpath('//input[@class=\"location-text\"]') # locating the seacrh bar by id\n",
    "search_item=input('Enter your search item') # creating the variable to take input from the user\n",
    "search_bar.send_keys(search_item)\n",
    "button=driver.find_element_by_xpath(\"//button[@id='search-button']\") # locating the button by x path\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "\n",
    "url = 'https://www.hostelworld.com/s?q=London,%20England&country=England&city=London&type=city&id=3&from=2022-01-03&to=2022-01-06&guests=2&page=1'\n",
    "path =r'C:\\Users\\rk1066\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "driver =webdriver.Chrome(path)\n",
    "driver.get(url)  \n",
    "time.sleep(2)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    " \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_tags = soup.find_all(class_=['div', 'property'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in property_tags:\n",
    "        data.append(tag.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
